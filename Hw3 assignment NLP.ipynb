{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a069444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85958004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\JYOTHIKA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JYOTHIKA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\JYOTHIKA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e28d446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = gutenberg.fileids()\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ebf1a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'austen-emma.txt'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files0 = files[0]\n",
    "files0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0bfb5a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bryant-stories.txt'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files5 = files[5]\n",
    "files5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d96b908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Book1 = gutenberg.raw(files0)\n",
    "Book2 = gutenberg.raw(files5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7ad2f369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\JYOTHIKA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JYOTHIKA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\JYOTHIKA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\JYOTHIKA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "\n",
    "# Downloads the 'punkt' tokenizer, which is used for sentence tokenization\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Downloads the 'stopwords' corpus, which is a list of commonly used words (such as \"the\", \"and\", etc.) that are usually removed from text during pre-processing\n",
    "nltk.download('stopwords')\n",
    "nltk_stops = nltk.corpus.stopwords.words('english')\n",
    "morestopwords = ['could','would','might','must','need','sha','wo','y',\"'s\",\"'d\",\"'ll\",\"'t\",\"'m\",\"'re\",\"'ve\"]\n",
    "stopwords = nltk_stops + morestopwords\n",
    " \n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76263b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1272c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenfile_B1 = nltk.word_tokenize(Book1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d942a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenfile_B2 = nltk.word_tokenize(Book2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc92d1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LowerCases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "129e4b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "Book1_lower = [i.lower() for i in tokenfile_B1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c2c8f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Book2_lower = [i.lower() for i in tokenfile_B2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac2537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "db4879dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Book1_words = [w for w in Book1_lower if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44abc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9d71c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Book2_words = [w for w in Book2_lower if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8e079b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "Book1_stop = [i for i in Book1_words if i in nltk_stops] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ccfdcbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Book2_stop = [j for j in Book2_words if j in nltk_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "069df161",
   "metadata": {},
   "outputs": [],
   "source": [
    "Book1_withoutstop = [i for i in Book1_words if i not in nltk_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "865c58a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Book2_withoutstop = [j for j in Book2_words if j not in nltk_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e8c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ea4f6b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "Book1_withoutstopLemma= [wnl.lemmatize(j) for j in Book1_withoutstop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549bbbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Analysis 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a60daadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'emma': 855, 'could': 836, 'would': 818, 'miss': 600, 'must': 566, 'harriet': 496, 'much': 484, 'said': 483, 'thing': 456, 'one': 451, ...})"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "fdist = FreqDist(Book1_withoutstopLemma)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6d1610d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'little': 596, 'said': 453, 'came': 191, 'one': 188, 'could': 172, 'king': 135, 'went': 122, 'would': 113, 'time': 110, 'great': 110, ...})"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist1 = FreqDist(Book2_withoutstopLemma)\n",
    "fdist1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "08393e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('emma', 855)\n",
      "('could', 836)\n",
      "('would', 818)\n",
      "('miss', 600)\n",
      "('must', 566)\n",
      "('harriet', 496)\n",
      "('much', 484)\n",
      "('said', 483)\n",
      "('thing', 456)\n",
      "('one', 451)\n",
      "('weston', 438)\n",
      "('every', 435)\n",
      "('think', 406)\n",
      "('well', 378)\n",
      "('elton', 378)\n",
      "('knightley', 373)\n",
      "('know', 365)\n",
      "('little', 359)\n",
      "('never', 358)\n",
      "('say', 341)\n",
      "('might', 325)\n",
      "('good', 313)\n",
      "('woodhouse', 308)\n",
      "('time', 303)\n",
      "('jane', 301)\n",
      "('quite', 282)\n",
      "('great', 263)\n",
      "('thought', 262)\n",
      "('friend', 257)\n",
      "('nothing', 252)\n",
      "('dear', 243)\n",
      "('always', 238)\n",
      "('man', 232)\n",
      "('fairfax', 232)\n",
      "('churchill', 229)\n",
      "('see', 225)\n",
      "('soon', 223)\n",
      "('may', 221)\n",
      "('shall', 217)\n",
      "('without', 214)\n",
      "('day', 209)\n",
      "('frank', 207)\n",
      "('first', 205)\n",
      "('like', 202)\n",
      "('father', 201)\n",
      "('sure', 201)\n",
      "('made', 199)\n",
      "('indeed', 196)\n",
      "('come', 195)\n",
      "('body', 193)\n"
     ]
    }
   ],
   "source": [
    "Book1_top50content = fdist.most_common(50)\n",
    "\n",
    "for i in Book1_top50content:\n",
    "    print (i)  # finding the top 50 content words for Book1 data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cc390e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('little', 596)\n",
      "('said', 453)\n",
      "('came', 191)\n",
      "('one', 188)\n",
      "('could', 172)\n",
      "('king', 135)\n",
      "('went', 122)\n",
      "('would', 113)\n",
      "('time', 110)\n",
      "('great', 110)\n",
      "('day', 108)\n",
      "('man', 105)\n",
      "('old', 102)\n",
      "('see', 99)\n",
      "('saw', 92)\n",
      "('like', 91)\n",
      "('come', 90)\n",
      "('mother', 90)\n",
      "('away', 90)\n",
      "('made', 89)\n",
      "('jackal', 84)\n",
      "('go', 78)\n",
      "('father', 78)\n",
      "('good', 77)\n",
      "('people', 76)\n",
      "('looked', 76)\n",
      "('tree', 75)\n",
      "('know', 75)\n",
      "('make', 71)\n",
      "('margery', 71)\n",
      "('thought', 70)\n",
      "('ran', 69)\n",
      "('big', 69)\n",
      "('boy', 68)\n",
      "('thing', 68)\n",
      "('child', 65)\n",
      "('two', 64)\n",
      "('home', 64)\n",
      "('put', 62)\n",
      "('every', 62)\n",
      "('door', 61)\n",
      "('way', 60)\n",
      "('lion', 60)\n",
      "('long', 58)\n",
      "('never', 58)\n",
      "('took', 57)\n",
      "('head', 57)\n",
      "('look', 57)\n",
      "('much', 57)\n",
      "('back', 56)\n"
     ]
    }
   ],
   "source": [
    "Book2_top50content = fdist1.most_common(50)\n",
    "\n",
    "for i in Book2_top50content:\n",
    "    print (i) # finding the top 50 content words for Book2 data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9622d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Analysis 2: doing the frequency distrubution of top 50 stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "aaedb8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 124 samples and 87421 outcomes>\n"
     ]
    }
   ],
   "source": [
    "fdist_stop = FreqDist(Book1_stop)\n",
    "print(fdist_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "dca1560f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 122 samples and 24471 outcomes>\n"
     ]
    }
   ],
   "source": [
    "fdist1_stop = FreqDist(Book2_stop)\n",
    "print(fdist1_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1e3643bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 5201)\n",
      "('to', 5181)\n",
      "('and', 4877)\n",
      "('of', 4284)\n",
      "('i', 3177)\n",
      "('a', 3124)\n",
      "('it', 2503)\n",
      "('her', 2448)\n",
      "('was', 2396)\n",
      "('she', 2336)\n",
      "('not', 2281)\n",
      "('in', 2173)\n",
      "('be', 1970)\n",
      "('you', 1967)\n",
      "('he', 1806)\n",
      "('that', 1805)\n",
      "('had', 1623)\n",
      "('but', 1441)\n",
      "('as', 1436)\n",
      "('for', 1346)\n",
      "('have', 1320)\n",
      "('is', 1241)\n",
      "('with', 1215)\n",
      "('very', 1202)\n",
      "('his', 1141)\n",
      "('at', 1030)\n",
      "('so', 968)\n",
      "('all', 841)\n",
      "('been', 755)\n",
      "('him', 749)\n",
      "('no', 741)\n",
      "('my', 728)\n",
      "('on', 689)\n",
      "('any', 654)\n",
      "('do', 652)\n",
      "('were', 599)\n",
      "('by', 569)\n",
      "('me', 563)\n",
      "('which', 556)\n",
      "('will', 556)\n",
      "('there', 548)\n",
      "('from', 546)\n",
      "('they', 540)\n",
      "('what', 536)\n",
      "('this', 526)\n",
      "('or', 494)\n",
      "('such', 489)\n",
      "('if', 485)\n",
      "('more', 466)\n",
      "('an', 463)\n"
     ]
    }
   ],
   "source": [
    "Book1_top50 = fdist_stop.most_common(50)\n",
    "\n",
    "for i in Book1_top50:\n",
    "    print (i) # top 50 stopwords for Book1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0acea236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 3449)\n",
      "('and', 2097)\n",
      "('to', 1158)\n",
      "('a', 1029)\n",
      "('he', 1016)\n",
      "('of', 818)\n",
      "('was', 720)\n",
      "('in', 640)\n",
      "('it', 614)\n",
      "('his', 551)\n",
      "('i', 533)\n",
      "('that', 531)\n",
      "('you', 462)\n",
      "('she', 409)\n",
      "('they', 396)\n",
      "('for', 339)\n",
      "('as', 327)\n",
      "('but', 308)\n",
      "('him', 296)\n",
      "('so', 295)\n",
      "('had', 295)\n",
      "('her', 291)\n",
      "('with', 284)\n",
      "('when', 271)\n",
      "('on', 262)\n",
      "('at', 261)\n",
      "('is', 247)\n",
      "('not', 242)\n",
      "('all', 239)\n",
      "('there', 218)\n",
      "('out', 195)\n",
      "('were', 194)\n",
      "('me', 188)\n",
      "('then', 185)\n",
      "('them', 185)\n",
      "('up', 178)\n",
      "('be', 169)\n",
      "('this', 161)\n",
      "('from', 158)\n",
      "('very', 157)\n",
      "('have', 154)\n",
      "('do', 150)\n",
      "('will', 147)\n",
      "('down', 134)\n",
      "('my', 126)\n",
      "('who', 125)\n",
      "('what', 124)\n",
      "('their', 112)\n",
      "('no', 110)\n",
      "('are', 104)\n"
     ]
    }
   ],
   "source": [
    "Book2_top50 = fdist1_stop.most_common(50)\n",
    "\n",
    "for i in Book2_top50:\n",
    "    print (i) # Finding the top 50 stop words for Book2 data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb305314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Analysis 3: analysing the top 50 frequency bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "16918594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', 'and'), 1882),\n",
       " (('.', \"''\"), 1157),\n",
       " ((\"''\", '``'), 959),\n",
       " ((';', 'and'), 867),\n",
       " (('to', 'be'), 605),\n",
       " ((',', \"''\"), 584),\n",
       " (('.', 'i'), 570),\n",
       " ((',', 'i'), 569),\n",
       " (('of', 'the'), 559),\n",
       " (('in', 'the'), 445),\n",
       " (('it', 'was'), 442),\n",
       " ((';', 'but'), 427),\n",
       " (('.', '``'), 416),\n",
       " (('.', 'she'), 413),\n",
       " (('i', 'am'), 394),\n",
       " ((',', 'that'), 360),\n",
       " (('!', '--'), 344),\n",
       " (('--', 'and'), 334),\n",
       " (('she', 'had'), 332),\n",
       " (('she', 'was'), 328),\n",
       " (('had', 'been'), 307),\n",
       " ((',', 'she'), 304),\n",
       " ((',', 'but'), 303),\n",
       " (('.', 'he'), 303),\n",
       " (('it', 'is'), 298),\n",
       " ((',', 'as'), 292),\n",
       " (('i', 'have'), 281),\n",
       " (('could', 'not'), 278),\n",
       " (('mr.', 'knightley'), 273),\n",
       " (('.', 'it'), 266),\n",
       " ((\"''\", 'said'), 265),\n",
       " ((',', 'to'), 264),\n",
       " (('``', 'i'), 261),\n",
       " (('of', 'her'), 260),\n",
       " (('--', 'i'), 257),\n",
       " (('.', 'the'), 251),\n",
       " ((',', '``'), 250),\n",
       " (('mrs.', 'weston'), 246),\n",
       " (('have', 'been'), 241),\n",
       " (('he', 'had'), 240),\n",
       " (('?', \"''\"), 238),\n",
       " ((',', 'in'), 237),\n",
       " (('to', 'the'), 237),\n",
       " (('do', 'not'), 235),\n",
       " (('--', 'but'), 232),\n",
       " ((',', 'the'), 226),\n",
       " (('and', 'the'), 224),\n",
       " (('he', 'was'), 222),\n",
       " (('would', 'be'), 215),\n",
       " ((',', 'it'), 214)]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "bigrams = [w for w in ngrams(Book1_lower, 2)]\n",
    "fdist_bigram = FreqDist(bigrams)\n",
    "fdist_bigram.most_common(50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c72dbcf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', 'and'), 917),\n",
       " (('.', '``'), 329),\n",
       " ((',', \"''\"), 299),\n",
       " (('the', 'little'), 296),\n",
       " ((\"''\", 'said'), 253),\n",
       " (('of', 'the'), 240),\n",
       " ((',', '``'), 238),\n",
       " (('in', 'the'), 232),\n",
       " (('!', \"''\"), 219),\n",
       " (('.', \"''\"), 218),\n",
       " (('.', 'the'), 211),\n",
       " (('said', 'the'), 191),\n",
       " (('and', 'the'), 182),\n",
       " (('to', 'the'), 176),\n",
       " ((\"''\", '``'), 146),\n",
       " (('?', \"''\"), 140),\n",
       " ((',', 'the'), 140),\n",
       " (('it', 'was'), 121),\n",
       " (('said', ','), 116),\n",
       " (('.', 'and'), 115),\n",
       " (('``', 'i'), 114),\n",
       " (('he', 'was'), 112),\n",
       " (('.', 'he'), 109),\n",
       " (('.', 'but'), 105),\n",
       " ((',', 'but'), 104),\n",
       " (('on', 'the'), 102),\n",
       " ((',', '--'), 100),\n",
       " (('and', 'he'), 100),\n",
       " ((',', 'he'), 99),\n",
       " (('a', 'little'), 97),\n",
       " ((';', 'and'), 84),\n",
       " ((\"''\", 'the'), 81),\n",
       " (('was', 'a'), 79),\n",
       " (('that', 'he'), 78),\n",
       " (('at', 'the'), 75),\n",
       " (('the', 'king'), 75),\n",
       " (('in', 'a'), 73),\n",
       " (('he', 'had'), 72),\n",
       " (('.', 'when'), 70),\n",
       " ((\"''\", 'and'), 67),\n",
       " (('when', 'the'), 67),\n",
       " (('when', 'he'), 67),\n",
       " (('he', 'said'), 66),\n",
       " (('--', \"''\"), 65),\n",
       " (('to', 'be'), 65),\n",
       " (('.', 'then'), 63),\n",
       " (('.', 'it'), 62),\n",
       " ((\"''\", 'he'), 62),\n",
       " (('from', 'the'), 61),\n",
       " ((',', 'to'), 61)]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "bigrams1 = [v for v in ngrams(Book2_lower, 2)]\n",
    "fdist1_bigram = FreqDist(bigrams1)\n",
    "fdist1_bigram.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20dedce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Analysis 4:analysing the top 50 frequency \n",
    "#bi grams with mutual information and a minimum frequency of 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1fa84d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('d', \"'ye\"), 14.964167861580208)\n",
      "(('sore', 'throat'), 14.089698743664066)\n",
      "(('brunswick', 'square'), 13.952195219914133)\n",
      "(('william', 'larkins'), 13.089698743664067)\n",
      "(('baked', 'apples'), 12.964167861580208)\n",
      "(('box', 'hill'), 12.736061789049367)\n",
      "(('sixteen', 'miles'), 12.613670614496076)\n",
      "(('maple', 'grove'), 12.594934051914489)\n",
      "(('hair', 'cut'), 12.063703535131124)\n",
      "(('south', 'end'), 11.96416786158021)\n",
      "(('colonel', 'campbell'), 11.412234161246522)\n",
      "(('protest', 'against'), 11.347496501131715)\n",
      "(('robert', 'martin'), 11.093935736550536)\n",
      "(('five', 'couple'), 10.841771230220482)\n",
      "(('vast', 'deal'), 10.76253400041056)\n",
      "(('ready', 'wit'), 10.652293431356767)\n",
      "(('donwell', 'abbey'), 10.519383018907314)\n",
      "(('musical', 'society'), 10.509114683453486)\n",
      "(('infinitely', 'superior'), 10.230813520966382)\n",
      "(('married', 'women'), 10.05727726597169)\n",
      "(('five', 'minutes'), 10.032714012931878)\n",
      "(('years', 'ago'), 9.9575041312992)\n",
      "(('three', 'months'), 9.941800048551755)\n",
      "(('depend', 'upon'), 9.928125111654678)\n",
      "(('ten', 'minutes'), 9.867013597351292)\n",
      "(('sat', 'down'), 9.795356480448604)\n",
      "(('hurrying', 'away'), 9.603101372785888)\n",
      "(('few', 'moments'), 9.558175501904373)\n",
      "(('few', 'minutes'), 9.41521754806233)\n",
      "(('lovely', 'woman'), 9.400399583128175)\n",
      "(('ten', 'years'), 9.372541630578041)\n",
      "(('last', 'night'), 9.368910380131174)\n",
      "(('sit', 'down'), 9.341844833355125)\n",
      "(('frank', 'churchill'), 9.284101419843205)\n",
      "(('few', 'lines'), 9.236247407017009)\n",
      "(('take', 'care'), 9.18038141066101)\n",
      "(('worthy', 'people'), 9.146544604068778)\n",
      "(('thrown', 'away'), 9.088528199956128)\n",
      "(('dare', 'say'), 9.06278824963678)\n",
      "(('three', 'times'), 9.04133572210267)\n",
      "(('next', 'week'), 9.035797538561239)\n",
      "(('few', 'weeks'), 9.01385498568056)\n",
      "(('how', 'd'), 9.01385498568056)\n",
      "(('great', 'deal'), 8.98664941695205)\n",
      "(('dear', 'madam'), 8.935801307930312)\n",
      "(('common', 'sense'), 8.934420518186156)\n",
      "(('jane', 'fairfax.'), 8.830083858371422)\n",
      "(('common', 'course'), 8.811038102680875)\n",
      "(('sitting', 'down'), 8.795356480448604)\n",
      "(('each', 'other'), 8.767770648776706)\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "from nltk.collocations import BigramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder1 = BigramCollocationFinder.from_words(Book1_lower)\n",
    "finder1.apply_freq_filter(5)\n",
    "scored = finder1.score_ngrams(bigram_measures.pmi)\n",
    "for bscore in scored[:50]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8b07487b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('herr', 'grupello'), 13.441491779304727)\n",
      "(('rid', 'hin'), 12.17845737347093)\n",
      "(('royal', 'robes'), 12.17845737347093)\n",
      "(('trundle-bed', 'boat'), 11.941418176170085)\n",
      "(('jack', 'rollaround'), 11.67595703294175)\n",
      "(('*', '*'), 11.534601183696205)\n",
      "(('new', 'orleans'), 11.239857918135073)\n",
      "(('clock', 'struck'), 11.166484731804855)\n",
      "(('shiny', 'acorn'), 11.108068045579532)\n",
      "(('christ', 'child'), 11.062980156050996)\n",
      "(('thou', 'art'), 10.997885127829111)\n",
      "(('small', 'rid'), 10.54102745285564)\n",
      "(('brother', 'rabbit'), 10.441491779304727)\n",
      "(('white', 'garraun'), 10.371102451413325)\n",
      "(('mr', 'alligator'), 10.348382374913244)\n",
      "(('red', 'hen'), 10.280336987490145)\n",
      "(('anything', 'else'), 10.119563684417363)\n",
      "(('god', 'save'), 10.062980156050994)\n",
      "(('fir', 'tree'), 9.888385547633698)\n",
      "(('prince', 'cherry'), 9.694257849684695)\n",
      "(('your', 'majesty'), 9.32047637834336)\n",
      "(('gingerbread', 'boy'), 9.303988255554792)\n",
      "(('field', 'mouse'), 9.210110574741021)\n",
      "(('country', 'mouse'), 9.197619630557828)\n",
      "(('ca', \"n't\"), 9.163507032004961)\n",
      "(('ai', \"n't\"), 9.16350703200496)\n",
      "(('city', 'mouse'), 9.104315910718427)\n",
      "(('long', 'ago'), 9.030969761148373)\n",
      "(('great', 'lizard'), 8.982060160667428)\n",
      "(('lazy', 'man'), 8.85652927858357)\n",
      "(('better', 'than'), 8.778526766582296)\n",
      "(('king', 'solomon'), 8.708137438690898)\n",
      "(('their', 'heads'), 8.69303054630069)\n",
      "(('most', 'beautiful'), 8.685269066457439)\n",
      "(('take', 'care'), 8.576421359390835)\n",
      "((\"'\", 'thin'), 8.566554329931499)\n",
      "((\"n't\", 'catch'), 8.447299998005553)\n",
      "(('an', \"'\"), 8.387489340259151)\n",
      "(('great', 'deal'), 8.30398825555479)\n",
      "(('wonder', 'if'), 8.286066347557528)\n",
      "(('took', 'hold'), 8.278453163447654)\n",
      "(('place', 'where'), 8.274240912160737)\n",
      "(('pretty', 'soon'), 8.232038413675774)\n",
      "(('old', 'alligator'), 8.090994532220593)\n",
      "(('could', 'hardly'), 8.074120713656196)\n",
      "(('old', 'woman'), 7.9654636501367335)\n",
      "(('should', 'eat'), 7.965295278188025)\n",
      "(('tiger', 'should'), 7.891001495840708)\n",
      "(('run', 'away'), 7.842723479058538)\n",
      "(('grey', 'man'), 7.826781935189516)\n"
     ]
    }
   ],
   "source": [
    "bigram_measures1 = nltk.collocations.BigramAssocMeasures()\n",
    "finder2 = BigramCollocationFinder.from_words(Book2_lower)\n",
    "finder2.apply_freq_filter(5)\n",
    "scored1 = finder2.score_ngrams(bigram_measures1.pmi)\n",
    "for bscore in scored1[:50]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0739eeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((',', 'and'), 0.009813071929504393)\n",
      "(('.', \"''\"), 0.006032797142633679)\n",
      "((\"''\", '``'), 0.005000391062908987)\n",
      "((';', 'and'), 0.004520687227885393)\n",
      "(('to', 'be'), 0.0031545741324921135)\n",
      "((',', \"''\"), 0.0030450765179758582)\n",
      "(('.', 'i'), 0.002972078108298355)\n",
      "((',', 'i'), 0.0029668639361785333)\n",
      "(('of', 'the'), 0.0029147222149803163)\n",
      "(('in', 'the'), 0.0023203065933206455)\n",
      "(('it', 'was'), 0.0023046640769611806)\n",
      "((';', 'but'), 0.002226451495163855)\n",
      "(('.', '``'), 0.002169095601845817)\n",
      "(('.', 'she'), 0.002153453085486352)\n",
      "(('i', 'am'), 0.00205438381520974)\n",
      "((',', 'that'), 0.001877101963135803)\n",
      "(('!', '--'), 0.0017936752092186563)\n",
      "(('--', 'and'), 0.0017415334880204396)\n",
      "(('she', 'had'), 0.0017311051437807962)\n",
      "(('she', 'was'), 0.0017102484553015095)\n",
      "(('had', 'been'), 0.0016007508407852543)\n",
      "((',', 'she'), 0.0015851083244257892)\n",
      "((',', 'but'), 0.0015798941523059676)\n",
      "(('.', 'he'), 0.0015798941523059676)\n",
      "(('it', 'is'), 0.0015538232917068592)\n",
      "((',', 'as'), 0.0015225382589879291)\n",
      "(('i', 'have'), 0.0014651823656698908)\n",
      "(('could', 'not'), 0.0014495398493104257)\n",
      "(('mr.', 'knightley'), 0.0014234689887113175)\n",
      "(('.', 'it'), 0.0013869697838725656)\n",
      "((\"''\", 'said'), 0.0013817556117527439)\n",
      "((',', 'to'), 0.0013765414396329223)\n",
      "(('``', 'i'), 0.0013608989232734572)\n",
      "(('of', 'her'), 0.0013556847511536356)\n",
      "(('--', 'i'), 0.0013400422347941705)\n",
      "(('.', 'the'), 0.0013087572020752405)\n",
      "((',', '``'), 0.001303543029955419)\n",
      "(('mrs.', 'weston'), 0.0012826863414761322)\n",
      "(('have', 'been'), 0.0012566154808770237)\n",
      "(('he', 'had'), 0.0012514013087572022)\n",
      "(('?', \"''\"), 0.0012409729645175588)\n",
      "((',', 'in'), 0.001235758792397737)\n",
      "(('to', 'the'), 0.001235758792397737)\n",
      "(('do', 'not'), 0.0012253304481580937)\n",
      "(('--', 'but'), 0.0012096879317986286)\n",
      "((',', 'the'), 0.0011784028990796985)\n",
      "(('and', 'the'), 0.0011679745548400552)\n",
      "(('he', 'was'), 0.0011575462106004119)\n",
      "(('would', 'be'), 0.0011210470057616603)\n",
      "((',', 'it'), 0.0011158328336418385)\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "from nltk.collocations import BigramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder1 = BigramCollocationFinder.from_words(Book1_lower)\n",
    "finder1.apply_freq_filter(5)\n",
    "scored = finder1.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored[:50]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "093ac156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((',', 'and'), 0.016485689630375378)\n",
      "(('.', '``'), 0.005914713073493456)\n",
      "((',', \"''\"), 0.005375377534877031)\n",
      "(('the', 'little'), 0.005321443981015389)\n",
      "((\"''\", 'said'), 0.00454839637566518)\n",
      "(('of', 'the'), 0.004314684308931396)\n",
      "((',', '``'), 0.004278728606356968)\n",
      "(('in', 'the'), 0.0041708614986336835)\n",
      "(('!', \"''\"), 0.0039371494318998996)\n",
      "(('.', \"''\"), 0.0039191715806126855)\n",
      "(('.', 'the'), 0.003793326621602186)\n",
      "(('said', 'the'), 0.003433769595857903)\n",
      "(('and', 'the'), 0.0032719689342729755)\n",
      "(('to', 'the'), 0.003164101826549691)\n",
      "((\"''\", '``'), 0.0026247662879332664)\n",
      "((',', 'the'), 0.0025168991802099814)\n",
      "(('?', \"''\"), 0.0025168991802099814)\n",
      "(('it', 'was'), 0.0021753200057529126)\n",
      "(('said', ','), 0.0020854307493168417)\n",
      "(('.', 'and'), 0.0020674528980296277)\n",
      "(('``', 'i'), 0.002049475046742413)\n",
      "(('he', 'was'), 0.002013519344167985)\n",
      "(('.', 'he'), 0.0019595857903063427)\n",
      "(('.', 'but'), 0.001887674385157486)\n",
      "((',', 'but'), 0.001869696533870272)\n",
      "(('on', 'the'), 0.0018337408312958435)\n",
      "((',', '--'), 0.0017977851287214151)\n",
      "(('and', 'he'), 0.0017977851287214151)\n",
      "((',', 'he'), 0.001779807277434201)\n",
      "(('a', 'little'), 0.0017438515748597727)\n",
      "((';', 'and'), 0.0015101395081259887)\n",
      "((\"''\", 'the'), 0.0014562059542643463)\n",
      "(('was', 'a'), 0.0014202502516899181)\n",
      "(('that', 'he'), 0.0014022724004027038)\n",
      "(('at', 'the'), 0.0013483388465410613)\n",
      "(('the', 'king'), 0.0013483388465410613)\n",
      "(('in', 'a'), 0.0013123831439666332)\n",
      "(('he', 'had'), 0.0012944052926794189)\n",
      "(('.', 'when'), 0.0012584495901049907)\n",
      "((\"''\", 'and'), 0.0012045160362433483)\n",
      "(('when', 'he'), 0.0012045160362433483)\n",
      "(('when', 'the'), 0.0012045160362433483)\n",
      "(('he', 'said'), 0.001186538184956134)\n",
      "(('--', \"''\"), 0.0011685603336689199)\n",
      "(('to', 'be'), 0.0011685603336689199)\n",
      "(('.', 'then'), 0.0011326046310944915)\n",
      "((\"''\", 'he'), 0.0011146267798072774)\n",
      "(('.', 'it'), 0.0011146267798072774)\n",
      "((',', 'to'), 0.0010966489285200633)\n",
      "(('from', 'the'), 0.0010966489285200633)\n"
     ]
    }
   ],
   "source": [
    "bigram_measures1 = nltk.collocations.BigramAssocMeasures()\n",
    "finder2 = BigramCollocationFinder.from_words(Book2_lower)\n",
    "finder2.apply_freq_filter(5)\n",
    "scored1 = finder2.score_ngrams(bigram_measures1.raw_freq)\n",
    "for bscore in scored1[:50]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2c7eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "644cf724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "296d737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_Book1 = list(ngrams(Book1_withoutstopLemma, 3))\n",
    "trigrams_Book2 = list(ngrams(Book2_withoutstopLemma, 3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "64e751d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_freq_Book1 = Counter(trigrams_Book1)\n",
    "trigram_freq_Book2 = Counter(trigrams_Book2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "23ecef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_freq_Book1_top50 = trigram_freq_Book1.most_common(50)\n",
    "trigram_freq_Book2_top50 = trigram_freq_Book2.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "dfb50894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dear', 'miss', 'woodhouse') : 24\n",
      "('oh', 'miss', 'woodhouse') : 14\n",
      "('poor', 'miss', 'taylor') : 11\n",
      "('said', 'frank', 'churchill') : 10\n",
      "('miss', 'woodhouse', 'would') : 8\n",
      "('fine', 'young', 'man') : 7\n",
      "('said', 'john', 'knightley') : 7\n",
      "('frank', 'churchill', 'miss') : 7\n",
      "('miss', 'smith', 'miss') : 7\n",
      "('every', 'body', 'else') : 6\n",
      "('miss', 'woodhouse', 'think') : 6\n",
      "('would', 'never', 'marry') : 5\n",
      "('great', 'deal', 'better') : 5\n",
      "('miss', 'bates', 'miss') : 5\n",
      "('miss', 'woodhouse', 'must') : 5\n",
      "('well', 'miss', 'woodhouse') : 5\n",
      "('said', 'emma', 'smiling') : 5\n",
      "('amiable', 'young', 'man') : 5\n",
      "('every', 'body', 'would') : 5\n",
      "('miss', 'bates', 'niece') : 5\n",
      "('dare', 'say', 'shall') : 5\n",
      "('miss', 'woodhouse', 'miss') : 5\n",
      "('bates', 'miss', 'fairfax') : 5\n",
      "('miss', 'woodhouse', 'said') : 5\n",
      "('think', 'miss', 'woodhouse') : 5\n",
      "('frank', 'churchill', 'come') : 4\n",
      "('tell', 'every', 'thing') : 4\n",
      "('every', 'thing', 'else') : 4\n",
      "('emma', 'could', 'feel') : 4\n",
      "('said', 'emma', 'laughing') : 4\n",
      "('talked', 'great', 'deal') : 4\n",
      "('woman', 'lovely', 'woman') : 4\n",
      "('never', 'saw', 'thing') : 4\n",
      "('shall', 'never', 'forget') : 4\n",
      "('dare', 'say', 'would') : 4\n",
      "('charming', 'young', 'man') : 4\n",
      "('jane', 'fairfax', 'could') : 4\n",
      "('saw', 'jane', 'fairfax') : 4\n",
      "('miss', 'fairfax', 'must') : 4\n",
      "('emma', 'could', 'imagine') : 4\n",
      "('emma', 'could', 'help') : 4\n",
      "('marrying', 'jane', 'fairfax') : 4\n",
      "('churchill', 'miss', 'woodhouse') : 4\n",
      "('know', 'miss', 'woodhouse') : 4\n",
      "('miss', 'bates', 'came') : 4\n",
      "('box', 'hill', 'party') : 4\n",
      "('miss', 'taylor', 'would') : 3\n",
      "('would', 'great', 'deal') : 3\n",
      "('well', 'said', 'emma') : 3\n",
      "('young', 'man', 'great') : 3\n"
     ]
    }
   ],
   "source": [
    "for trigram, freq in trigram_freq_Book1_top50:\n",
    "    print(trigram, \":\", freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e34fbb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('little', 'gingerbread', 'boy') : 25\n",
      "('little', 'fir', 'tree') : 25\n",
      "('little', 'red', 'hen') : 22\n",
      "('said', 'little', 'jackal') : 18\n",
      "('little', 'jack', 'rollaround') : 12\n",
      "('little', 'red', 'man') : 10\n",
      "('little', 'old', 'woman') : 9\n",
      "('little', 'country', 'mouse') : 9\n",
      "('small', 'rid', 'hin') : 9\n",
      "('mammy', 'said', 'epaminondas') : 8\n",
      "('old', 'woman', 'little') : 7\n",
      "('woman', 'little', 'old') : 7\n",
      "('little', 'old', 'man') : 7\n",
      "('little', 'brother', 'rabbit') : 7\n",
      "('said', 'little', 'red') : 6\n",
      "('run', 'run', 'fast') : 6\n",
      "('run', 'fast', 'ca') : 6\n",
      "('fast', 'ca', 'catch') : 6\n",
      "('ca', 'catch', 'gingerbread') : 6\n",
      "('catch', 'gingerbread', 'man') : 6\n",
      "('could', 'catch', 'little') : 6\n",
      "('god', 'save', 'said') : 6\n",
      "('old', 'white', 'garraun') : 6\n",
      "('said', 'little', 'tulip') : 5\n",
      "('upon', 'time', 'little') : 5\n",
      "('gingerbread', 'boy', 'ran') : 5\n",
      "('run', 'away', 'little') : 5\n",
      "('away', 'little', 'old') : 5\n",
      "('little', 'city', 'mouse') : 5\n",
      "('little', 'field', 'mouse') : 5\n",
      "('mr', 'alligator', 'kind') : 5\n",
      "('eat', 'set', 'free') : 5\n",
      "('stood', 'quite', 'still') : 4\n",
      "('said', 'goose', 'said') : 4\n",
      "('goose', 'said', 'duck') : 4\n",
      "('ran', 'fast', 'could') : 4\n",
      "('catch', 'little', 'gingerbread') : 4\n",
      "('eat', 'little', 'gingerbread') : 4\n",
      "('old', 'man', 'cow') : 4\n",
      "('gingerbread', 'boy', 'jumped') : 4\n",
      "('little', 'jackal', 'know') : 4\n",
      "('roll', 'around', 'roll') : 4\n",
      "('around', 'roll', 'around') : 4\n",
      "('madrid', 'see', 'king') : 4\n",
      "('idea', 'said', 'little') : 4\n",
      "('last', 'one', 'day') : 4\n",
      "('little', 'small', 'rid') : 4\n",
      "('came', 'along', 'home') : 4\n",
      "('epaminondas', 'ai', 'got') : 4\n",
      "('ai', 'got', 'sense') : 4\n"
     ]
    }
   ],
   "source": [
    "for trigram, freq in trigram_freq_Book2_top50:\n",
    "    print(trigram, \":\", freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e4624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Analysis 5: averaged perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "69d76810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\JYOTHIKA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "tagged_token = nltk.pos_tag(Book1_withoutstopLemma)\n",
    "Book1_adjs = [w for w,p in tagged_token if p=='JJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "9640b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_token1 = nltk.pos_tag(Book2_withoutstopLemma)\n",
    "Book2_adjs = [v for v,q in tagged_token1 if q=='JJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3ac59cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('harriet', 417)\n",
      "('little', 326)\n",
      "('much', 315)\n",
      "('good', 303)\n",
      "('great', 263)\n",
      "('miss', 254)\n",
      "('emma', 220)\n",
      "('young', 192)\n",
      "('dear', 153)\n",
      "('sure', 150)\n",
      "('many', 138)\n",
      "('poor', 136)\n",
      "('last', 121)\n",
      "('u', 121)\n",
      "('happy', 120)\n",
      "('first', 104)\n",
      "('wish', 92)\n",
      "('possible', 84)\n",
      "('old', 83)\n",
      "('give', 82)\n",
      "('present', 76)\n",
      "('subject', 73)\n",
      "('able', 71)\n",
      "('whole', 71)\n",
      "('short', 67)\n",
      "('true', 64)\n",
      "('general', 61)\n",
      "('ready', 61)\n",
      "('enough', 58)\n",
      "('know', 57)\n",
      "('superior', 57)\n",
      "('bad', 56)\n",
      "('equal', 55)\n",
      "('right', 55)\n",
      "('frank', 55)\n",
      "('natural', 51)\n",
      "('usual', 50)\n",
      "('next', 50)\n",
      "('weston', 50)\n",
      "('long', 48)\n",
      "('oh', 48)\n",
      "('particular', 48)\n",
      "('afraid', 47)\n",
      "('agreeable', 47)\n",
      "('mean', 47)\n",
      "('full', 46)\n",
      "('come', 45)\n",
      "('strong', 44)\n",
      "('real', 42)\n",
      "('different', 41)\n"
     ]
    }
   ],
   "source": [
    "fadjdist = FreqDist(Book1_adjs)\n",
    "Book1_top50adj = fadjdist.most_common(50)\n",
    "\n",
    "for k in Book1_top50adj:\n",
    "    print (k) # top 50 adjectives for true data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "0ecfb9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('little', 578)\n",
      "('great', 110)\n",
      "('old', 102)\n",
      "('good', 76)\n",
      "('big', 69)\n",
      "('last', 55)\n",
      "('beautiful', 43)\n",
      "('white', 42)\n",
      "('poor', 41)\n",
      "('red', 41)\n",
      "('many', 41)\n",
      "('nightingale', 37)\n",
      "('tree', 31)\n",
      "('tiny', 29)\n",
      "('small', 28)\n",
      "('next', 28)\n",
      "('u', 28)\n",
      "('gingerbread', 27)\n",
      "('hard', 27)\n",
      "('fir', 26)\n",
      "('long', 24)\n",
      "('dear', 24)\n",
      "('strong', 24)\n",
      "('much', 24)\n",
      "('happy', 24)\n",
      "('new', 23)\n",
      "('jackal', 21)\n",
      "('open', 20)\n",
      "('green', 19)\n",
      "('sweet', 18)\n",
      "('give', 17)\n",
      "('whole', 17)\n",
      "('terrible', 17)\n",
      "('know', 17)\n",
      "('wonderful', 17)\n",
      "('fine', 16)\n",
      "('fox', 16)\n",
      "('oh', 16)\n",
      "('saw', 16)\n",
      "('full', 15)\n",
      "('first', 15)\n",
      "('light', 15)\n",
      "('servant', 15)\n",
      "('right', 15)\n",
      "('young', 15)\n",
      "('lazy', 15)\n",
      "('soft', 14)\n",
      "('dead', 14)\n",
      "('bad', 14)\n",
      "('brahmin', 14)\n"
     ]
    }
   ],
   "source": [
    "fadjdist1 = FreqDist(Book2_adjs)\n",
    "Book2_top50adj = fadjdist1.most_common(50)\n",
    "\n",
    "for k in Book2_top50adj:\n",
    "    print (k) # top 50 adjectives for fake data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c60f9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c75be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A)\n",
    "#Since tokenization is an essential stage in text processing, \n",
    "#I decided to use it to separate the text into distinct words.\n",
    "#To guarantee consistency in word frequencies and to standardize \n",
    "#the text, lowercasing was used.\n",
    "#In order to eliminate popular terms that usually have little meaning, \n",
    "#stopwords were eliminated.\n",
    "#Words with similar meanings can be grouped together by using \n",
    "#lemmatization, which reduces inflected words to their basic form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceac302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B)\n",
    "# Proper nouns or domain-specific terms that might not be, \n",
    "#pertinent to the study could be one potential issue with the word lists.\n",
    "#Common bigrams, which appear frequently but may,\n",
    "#not necessarily have important meaning, may be found in the bigram lists.\n",
    "#We could remove stopwords from bigrams or use other, \n",
    "#steps to find significant collocations in order to enhance the bigram lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e2ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C)\n",
    "#Regardless of how mutually informative their word pairs are, \n",
    "#the top 50 bigrams by frequency show the most frequently occurring pairs\n",
    "#of neighboring words in the text.\n",
    "#Conversely, the top 50 bigrams based on Mutual Information \n",
    "#show word pairings that co-occur more frequently than one would \n",
    "#anticipate by chance, indicating a greater relationship between the terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f12b158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d)\n",
    "#We may add domain-specific stopwords or exclude frequent words that,\n",
    "#are unrelated to our analysis if we make changes to the stopwords list.\n",
    "#To weed out less instructive bigrams, we might also try varying the\n",
    "#frequency criteria for bigrams or use other collocation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5c6878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#e)\n",
    "#Trigrams can provide further light on the relationships and context of words.\n",
    "#For a more thorough understanding of the text, \n",
    "#we can run top trigram lists using techniques similar to those \n",
    "#used for bigrams and incorporate them into the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d683cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca973273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3)\n",
    "\n",
    "#A)\n",
    "#I would like to compare the language and writing style of \"austen-emma.txt\" \n",
    "#(from Jane Austen's \"Emma\") with \"bryant-stories.txt\" \n",
    "#(a compilation of William Cullen Bryant's short stories). \n",
    "#I want to specifically look into the differences \n",
    "#between these two texts' word choices and the frequency of particular bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee5dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B)\n",
    "\n",
    "#Word Distributions:\n",
    "\n",
    "#Words pertaining to relationships, social interactions, and manners \n",
    "#are likely to be used frequently in \"austen-emma.txt,\" \n",
    "#which is not surprising given Austen's emphasis on \n",
    "#love themes and conventional conventions.\n",
    "#In \"bryant-stories.txt,\" we may find a broader language pertaining to the \n",
    "#natural world, rural living, and landscapes, \n",
    "#considering Bryant's standing as a nature writer and poet.\n",
    "\n",
    "#Bigram Rates:\n",
    "\n",
    "#Studying the frequent bigrams in \"austen-emma.txt,\" \n",
    "#we could come across word pairs like \"Mr. Knightley,\" \"Miss Woodhouse,\" \n",
    "#or \"social engagements,\" which correspond to the social contexts and \n",
    "#interpersonal interactions that the book describes.\n",
    "#By contrast, many bigrams in \"bryant-stories.txt\" may \n",
    "#consist of word combinations such as \"old oak,\" \"forest trees,\" or \n",
    "#\"crystal stream,\" demonstrating Bryant's preference for environmental \n",
    "#imagery and descriptive language.\n",
    "\n",
    "#Bigram Exchange Data:\n",
    "\n",
    "#Strong correlations between terms like \"love\" and \"marriage,\" \n",
    "#\"manners\" and \"society,\" or \"romantic\" and \"entanglements\" may be \n",
    "#shown by Mutual Information scores for bigrams in \"austen-emma.txt,\" \n",
    "#suggesting recurrent themes and motifs in Austen's writing.\n",
    "#Relevant word pairs, like \"silent woods,\" \"rustling leaves,\" or\n",
    "#\"rippling brook,\" may be highlighted by Mutual Information scores for \n",
    "#\"bryant-stories.txt,\" highlighting Bryant's poetic depictions of the natural world.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
